{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cafc6bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src='ntu_logo.png' >\n",
    "\n",
    "\n",
    "<div align='center' ><font size='6'>Regression through Neural Network by neuralnet & keras</font></div>\n",
    "\n",
    "__ZHOU DUO, G2103160J__\n",
    "\n",
    "__SPMS, NTU__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed99702",
   "metadata": {},
   "source": [
    "# Introduction of Neural Network\n",
    "\n",
    "## What is Neural Network\n",
    "* Neural networks are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "* Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.Based on this characteristic, choose to use neural network regression analysis.\n",
    "\n",
    "\n",
    "## How do neural networks work\n",
    "* Think of each individual node as its own linear regression model, composed of input data, weights, a bias (or threshold), and an output. The formula would look something like this:\n",
    "$$ activation\\ function: \\sum_{i=1}^{m} w_i\\cdot x_i +bias $$\n",
    "\n",
    "* Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function(mostly ReLU this assignment)which determines the output. \n",
    "$$ Rectified\\ Linear\\ Unit\\ = max(0,x) $$\n",
    "\n",
    "## How do neural networks work\n",
    "### Activation function\n",
    "\n",
    "* This assignment used perceptrons to illustrate some mathematics at play here,  neural networks leverage ReLU neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "* If that output exceeds a given threshold, it activates the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. \n",
    "\n",
    "###  Cost function\n",
    "* We adjust the weights or the threshold and achieve different outcomes from the model. When we observe one decision, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "\n",
    "* As we start to think about more practical use cases for classification, we’ll leverage supervised learning, or labeled datasets, to train the algorithm. As we train the model, we’ll want to evaluate its accuracy using a cost (or loss) function. This is also commonly referred to as the mean squared error (MSE). Sometimes, we use SSE,too. In the equation below:\n",
    "\n",
    "$$ SSE = \\sum_{i=1}^{n}w_i(y_i-\\hat{y_i})^2 $$\n",
    "$$ MSE = \\frac{1}{m}\\sum_{i=1}^{m}(y_i-\\hat{y_i})^2 $$\n",
    "\n",
    "* Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation. As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum. The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors (or minimize the cost function). With each training example, the parameters of the model adjust to gradually converge at the minimum.  \n",
    "\n",
    "\n",
    "\n",
    "* This assignment use MLP to do some regression analysis. The assginment 2 will introduce keras to do regression prodiction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25828d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DNN in concrete dataset\n",
    "\n",
    "* chapter7. neural network\n",
    "\n",
    "* refer to p231 in ML with R\n",
    "\n",
    "## Step 1 – collecting data\n",
    "\n",
    "* For this analysis, we will utilize data on the compressive strength of concrete donated to the UCI Machine Learning Data Repository (https://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength) by I-Cheng Yeh. The concrete dataset contains 1,030 examples of concrete with eight features describing the components used in the mixture.\n",
    "\n",
    "* These features are thought to be related to the final compressive strength and they include the amount (in kilograms per cubic meter) of cement, slag, ash, water, superplasticizer, coarse aggregate, and fine aggregate used in the product in addition to the aging time (measured in days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4ca92b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'/Users/clause/Documents/NTUPresentationBeamer-master'"
      ],
      "text/latex": [
       "'/Users/clause/Documents/NTUPresentationBeamer-master'"
      ],
      "text/markdown": [
       "'/Users/clause/Documents/NTUPresentationBeamer-master'"
      ],
      "text/plain": [
       "[1] \"/Users/clause/Documents/NTUPresentationBeamer-master\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "getwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d675ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2 – exploring and preparing the data\n",
    "\n",
    "* As usual, we'll begin our analysis by loading the data into an R object using the read.csv() function, and confirming that it matches the expected structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdca34e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1030 obs. of  9 variables:\n",
      " $ cement      : num  540 540 332 332 199 ...\n",
      " $ slag        : num  0 0 142 142 132 ...\n",
      " $ ash         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ water       : num  162 162 228 228 192 228 228 228 228 228 ...\n",
      " $ superplastic: num  2.5 2.5 0 0 0 0 0 0 0 0 ...\n",
      " $ coarseagg   : num  1040 1055 932 932 978 ...\n",
      " $ fineagg     : num  676 676 594 594 826 ...\n",
      " $ age         : int  28 28 270 365 360 90 365 28 28 28 ...\n",
      " $ strength    : num  80 61.9 40.3 41 44.3 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    strength    \n",
       " Min.   : 2.33  \n",
       " 1st Qu.:23.71  \n",
       " Median :34.45  \n",
       " Mean   :35.82  \n",
       " 3rd Qu.:46.13  \n",
       " Max.   :82.60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concrete <- read.csv(\"Concrete.csv\")\n",
    "str(concrete)\n",
    "summary(concrete[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034adce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The nine variables in the data frame correspond to the eight features and one outcome we expected, although a problem has become apparent. Neural networks work best when the input data are scaled to a narrow range around zero, and here, we see values ranging anywhere from zero up to over a thousand.\n",
    "\n",
    "\n",
    "\n",
    "* Typically, the solution to this problem is to rescale the data with a normalizing or standardization function. If the data follow a bell-shaped curve , then it may make sense to use standardization via R's built-in scale() function. \n",
    "\n",
    "$$ nomalized = \\frac{x-min(x)}{max(x)-min(x)} $$\n",
    "\n",
    "* On the other hand, if the data follow a uniform distribution or are severely nonnormal, then normalization to a 0-1 range may be more appropriate. In this case, we'll use the latter to normalize our dataset.\n",
    "\n",
    "* To confirm that the normalization worked, we can see that the minimum and maximum strength are now 0 and 1, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5bd0ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       " 0.0000  0.2664  0.4001  0.4172  0.5457  1.0000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalizing\n",
    "normalize <- function(x) {\n",
    "  return((x - min(x)) / (max(x) - min(x)))\n",
    "}\n",
    "concrete_norm <- as.data.frame(lapply(concrete, normalize))\n",
    "summary(concrete_norm[[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70d24e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Following Yeh's precedent in the original publication, we will partition the data into a training set with 75 percent of the examples and a testing set with 25 percent. The CSV file we used was already sorted in random order, so we simply need to divide it into two portions:\n",
    "\n",
    "\n",
    "* We'll use the training dataset to build the neural network and the testing dataset to evaluate how well the model generalizes to future results. As it is easy to overfit a neural network, this step is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c72d074",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t773 obs. of  9 variables:\n",
      " $ cement      : num  1 1 0.526 0.526 0.221 ...\n",
      " $ slag        : num  0 0 0.396 0.396 0.368 ...\n",
      " $ ash         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ water       : num  0.321 0.321 0.848 0.848 0.561 ...\n",
      " $ superplastic: num  0.0776 0.0776 0 0 0 ...\n",
      " $ coarseagg   : num  0.695 0.738 0.381 0.381 0.516 ...\n",
      " $ fineagg     : num  0.206 0.206 0 0 0.581 ...\n",
      " $ age         : num  0.0742 0.0742 0.739 1 0.9863 ...\n",
      " $ strength    : num  0.967 0.742 0.473 0.482 0.523 ...\n",
      "'data.frame':\t257 obs. of  9 variables:\n",
      " $ cement      : num  0.639 0.639 0.409 0.541 0.541 ...\n",
      " $ slag        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ ash         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ water       : num  0.513 0.513 0.513 0.505 0.505 ...\n",
      " $ superplastic: num  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ coarseagg   : num  0.715 0.901 0.881 0.779 0.779 ...\n",
      " $ fineagg     : num  0.364 0.477 0.452 0.401 0.401 ...\n",
      " $ age         : num  0.0742 0.0165 0.0742 0.0165 0.0742 ...\n",
      " $ strength    : num  0.437 0.114 0.251 0.235 0.368 ...\n"
     ]
    }
   ],
   "source": [
    "# dividing into train & test\n",
    "concrete_train <- concrete_norm[1:773, ]\n",
    "concrete_test <- concrete_norm[774:1030, ]\n",
    "str(concrete_train)\n",
    "str(concrete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8d810",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3 – training a model on the data\n",
    "\n",
    "* To model the relationship between the ingredients used in concrete and the strength of the finished product, we will use a multilayer feedforward neural network. The neuralnet package by Stefan Fritsch and Frauke Guenther provides a standard and easy-to-use implementation of such networks. It also offers a function to plot the network topology. \n",
    "\n",
    "\n",
    "* We'll begin by training the simplest multilayer feedforward network with only a single hidden node to explore the relationship between strenth and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13794d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check & install packages\n",
    "requiredPackages <- 'neuralnet'\n",
    "if (length(setdiff(requiredPackages, rownames(installed.packages()))) > 0) {\n",
    "  install.packages(setdiff(requiredPackages, rownames(installed.packages())))  \n",
    "}\n",
    "library('neuralnet')\n",
    "concrete_model <- neuralnet(strength ~ .,\n",
    "                            data = concrete_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646ae32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can then visualize the network topology using the plot() function on the resulting model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b93780",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(concrete_model) # here is some thing wrong, jupyter cannot shou the plot, so I use the plot showed by Rstudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb921894",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"Rplot0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57059622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In this simple model, there is one input node for each of the eight features, followed by a single hidden node and a single output node that predicts the concrete strength. The weights for each of the connections are also depicted, as are the bias terms (indicated by the nodes labeled with the number 1). \n",
    "$$ activation\\ function: \\sum_{i=1}^{m} w_i\\cdot x_i +bias $$\n",
    "* The bias terms are numeric constants that allow the value at the indicated nodes to be shifted upward or downward, much like the intercept in a linear equation.\n",
    "\n",
    "\n",
    "* At the bottom of the figure, R reports the number of training steps and an error measure called the Sum of Squared Errors (SSE), is the sum of the squared predicted minus actual values. \n",
    "$$ SSE = \\sum_{i=1}^{n}w_i(y_i-\\hat{y_i})^2 $$\n",
    "* A lower SSE implies better predictive performance. This is helpful for estimating the model's performance on the training data, but tells us little about how it will perform on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab047cf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4 – evaluating model performance\n",
    "* The network topology diagram gives us a peek into the black box of the ANN, but it doesn't provide much information about how well the model fits future data. To generate predictions on the test dataset, we can use the compute() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e24d1fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 2\n",
      " $ neurons   :List of 2\n",
      "  ..$ : num [1:257, 1:9] 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  .. ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. .. ..$ : chr [1:257] \"774\" \"775\" \"776\" \"777\" ...\n",
      "  .. .. ..$ : chr [1:9] \"\" \"cement\" \"slag\" \"ash\" ...\n",
      "  ..$ : num [1:257, 1:2] 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  .. ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. .. ..$ : chr [1:257] \"774\" \"775\" \"776\" \"777\" ...\n",
      "  .. .. ..$ : NULL\n",
      " $ net.result: num [1:257, 1] 0.39 0.244 0.25 0.227 0.332 ...\n",
      "  ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. ..$ : chr [1:257] \"774\" \"775\" \"776\" \"777\" ...\n",
      "  .. ..$ : NULL\n"
     ]
    }
   ],
   "source": [
    "model_results <- compute(concrete_model, concrete_test[1:8])\n",
    "str(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982738bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The compute() function works a bit differently from the predict() functions we've used so far. It returns a list with two components: $neurons$, which stores the neurons for each layer in the network, and $net.result$, which stores the predicted values. We'll want the latter:\n",
    "\n",
    "\n",
    "\n",
    "* Because this is a numeric prediction problem rather than a classification problem, we cannot use a confusion matrix to examine model accuracy. Instead, we must measure the correlation between our predicted concrete strength and the true value. This provides insight into the strength of the linear association between the two variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593aa830",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 1 × 1 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>0.7203357</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 1 × 1 of type dbl\n",
       "\\begin{tabular}{l}\n",
       "\t 0.7203357\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 1 × 1 of type dbl\n",
       "\n",
       "| 0.7203357 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]     \n",
       "[1,] 0.7203357"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_strength <- model_results$net.result\n",
    "cor(predicted_strength, concrete_test$strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6a50c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Correlations close to 1 indicate strong linear relationships between two variables. Therefore, the correlation here of about 0.723 indicates a fairly strong relationship. This implies that our model is doing a fairly good job, even with only a single hidden node.\n",
    "\n",
    "\n",
    "\n",
    "* Given that we only used one hidden node, it is likely that we can improve the performance of our model. Let's try to do a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d9d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5 – improving model performance\n",
    "* As networks with more complex topologies are capable of learning more difficult concepts, let's see what happens when we increase the number of hidden nodes to five. We use the neuralnet() function as before, but add the loop to set the hidden nodes =1,2,3,4,5 respectively :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55db5ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [,1]\n",
      "[1,] 0.7201878\n",
      "         [,1]\n",
      "[1,] 0.767318\n",
      "         [,1]\n",
      "[1,] 0.797926\n",
      "          [,1]\n",
      "[1,] 0.7976516\n",
      "          [,1]\n",
      "[1,] 0.7998474\n"
     ]
    }
   ],
   "source": [
    "for (i in c(1,2,3,4,5)) {\n",
    "  hidden <- i\n",
    "  concrete_model <- neuralnet(strength ~ .,\n",
    "                            data = concrete_train, hidden = hidden)\n",
    "  model_results <- compute(concrete_model, concrete_test[1:8])\n",
    "  predicted_strength <- model_results$net.result \n",
    "  output <- cor(predicted_strength, concrete_test$strength)\n",
    "  print(output)\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f805f34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When hidden nodes = 5, correlation get the highest level. Plotting the network again, we see a drastic increase in the number of connections. We can see how this impacted the performance as follows:\n",
    "\n",
    "<img src=\"Rplot1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed265c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Notice that the reported error (measured again by SSE) has been reduced from 5.67 in the previous model to 1.67 here. Additionally, the number of training steps rose from 1317 to 27791, which should come as no surprise given how much more complex the model has become. More complex networks take many more iterations to find the optimal weights.\n",
    "\n",
    "* Applying the same steps to compare the predicted values to the true values, we now obtain a correlation around 0.80, which is a considerable improvement over the previous result of 0.72 with a single hidden node:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d7a73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DNN in qsar_fish_toxicity dataset\n",
    "\n",
    "* refer to p231 in ML with R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f625b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1 – collecting data\n",
    "\n",
    "* For this analysis, we will utilize data on the compressive strength of concrete donated to the UCI Machine Learning Data Repository (https://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity).\n",
    "\n",
    "* This dataset was used to develop quantitative regression QSAR models to predict acute aquatic toxicity towards the fish Pimephales promelas (fathead minnow) on a set of 908 chemicals. LC50 data, which is the concentration that causes death in 50% of test fish over a test duration of 96 hours, was used as model response. \n",
    "\n",
    "* The model comprised 6 molecular descriptors: MLOGP (molecular properties), CIC0 (information indices), GATS1i (2D autocorrelations), NdssC (atom-type counts), NdsCH ((atom-type counts), SM1_Dz(Z) (2D matrix-based descriptors). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679edff3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2 – exploring and preparing the data\n",
    "\n",
    "* As usual, we'll begin our analysis by loading the data into an R object using the read.csv() function, and confirming that it matches the expected structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8259dd0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t908 obs. of  7 variables:\n",
      " $ V1: num  3.26 2.19 2.12 3.03 2.09 ...\n",
      " $ V2: num  0.829 0.58 0.638 0.331 0.827 0.331 0 0 0.499 0.134 ...\n",
      " $ V3: num  1.676 0.863 0.831 1.472 0.86 ...\n",
      " $ V4: int  0 0 0 1 0 0 0 1 0 0 ...\n",
      " $ V5: int  1 0 0 0 0 0 0 0 0 0 ...\n",
      " $ V6: num  1.45 1.35 1.35 1.81 1.89 ...\n",
      " $ V7: num  3.77 3.12 3.53 3.51 5.39 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       V7       \n",
       " Min.   :0.053  \n",
       " 1st Qu.:3.152  \n",
       " Median :3.987  \n",
       " Mean   :4.064  \n",
       " 3rd Qu.:4.907  \n",
       " Max.   :9.612  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QSAR <- read.csv(\"qsar_fish_toxicity.csv\", header = F, sep = ';')\n",
    "str(QSAR)\n",
    "summary(QSAR[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a269b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The 7 variables in the data frame correspond to the 6 features and one outcome we expected, although a problem has become apparent. Neural networks work best when the input data are scaled to a narrow range around zero.\n",
    "\n",
    "\n",
    "\n",
    "* We stilly use the solution to this problem is to rescale the data with a normalizing or standardization function. Normalization to a 0-1 range may be more appropriate.\n",
    "\n",
    "$$ nomalized = \\frac{x-min(x)}{max(x)-min(x)} $$\n",
    "\n",
    "* To confirm that the normalization worked, we can see that the minimum and maximum strength are now 0 and 1, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a36ccf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       " 0.0000  0.3242  0.4116  0.4196  0.5078  1.0000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalizing\n",
    "normalize <- function(x) {\n",
    "  return((x - min(x)) / (max(x) - min(x)))\n",
    "}\n",
    "QSAR_norm <- as.data.frame(lapply(QSAR, normalize))\n",
    "summary(QSAR_norm[[7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3eb204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Because the dataset is relatively small, we will partition the data into a training set with 67 percent of the examples and a testing set with 33 percent. The CSV file we used was already sorted in random order, so we simply need to divide it into two portions:\n",
    "\n",
    "\n",
    "* We'll use the training dataset to build the neural network and the testing dataset to evaluate how well the model generalizes to future results. As it is easy to overfit a neural network, this step is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49037aaa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# dividing into train & test\n",
    "QSAR_train <- QSAR_norm[1:606, ]\n",
    "QSAR_test <- QSAR_norm[607:908, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcb912",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3 – training a model on the data & evaluating model performance\n",
    "\n",
    "* To model the relationship between the ingredients of chemicals acute aquatic toxicity towards the fish Pimephales promelas, we will use a multilayer feedforward neural network. The neuralnet package by Stefan Fritsch and Frauke Guenther provides a standard and easy-to-use implementation of such networks. It also offers a function to plot the network topology. \n",
    "\n",
    "\n",
    "\n",
    "* As networks with more complex topologies are capable of learning more difficult concepts, we increase the number of hidden nodes to five. We use the neuralnet() function as before, but add the loop to set the hidden layer =1,2,3,4,5 respectively :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233471ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [,1]\n",
      "[1,] 0.745825\n",
      "          [,1]\n",
      "[1,] 0.8044063\n",
      "          [,1]\n",
      "[1,] 0.7771851\n",
      "         [,1]\n",
      "[1,] 0.763678\n",
      "         [,1]\n",
      "[1,] 0.759999\n"
     ]
    }
   ],
   "source": [
    "# neuralnet modeling, try hidden layers = 1-5\n",
    "library('neuralnet')\n",
    "for (i in c(1,2,3,4,5)) {\n",
    "  QSAR_model <- neuralnet(V7 ~ V1 + V2 + V3 + V4 +V5 + V6,\n",
    "                          data = QSAR_train, hidden = hidden)\n",
    "  # visualization\n",
    "  plot(QSAR_model)\n",
    "  model_results <- neuralnet::compute(QSAR_model, QSAR_test[1:6])\n",
    "  predicted_strength <- model_results$net.result\n",
    "  output <- cor(predicted_strength, QSAR_test$V7)\n",
    "  print(output)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a9896",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* It can be seen that the performance of the single-layer perceptron is hidden node = 2, with corelation =0.80\n",
    "\n",
    "* We can then visualize the network topology using the plot() function on the resulting model object:\n",
    "\n",
    "<img src='Rplot5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86136bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4 – increase layers to improve model performance\n",
    "\n",
    "* Then, we try to increase the hidden layers to 2, nodes from (2,1) to (4,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a815a6",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1          [,1]\n",
      "[1,] 0.7877105\n",
      "2 2          [,1]\n",
      "[1,] 0.8005624\n",
      "2 3          [,1]\n",
      "[1,] 0.7998047\n",
      "2 4          [,1]\n",
      "[1,] 0.7979071\n",
      "2 5          [,1]\n",
      "[1,] 0.7854103\n",
      "3 1          [,1]\n",
      "[1,] 0.7973076\n",
      "3 2          [,1]\n",
      "[1,] 0.7912572\n",
      "3 3          [,1]\n",
      "[1,] 0.8108005\n",
      "3 4          [,1]\n",
      "[1,] 0.7966013\n",
      "3 5          [,1]\n",
      "[1,] 0.7903481\n",
      "4 1          [,1]\n",
      "[1,] 0.7909494\n",
      "4 2          [,1]\n",
      "[1,] 0.7939436\n",
      "4 3          [,1]\n",
      "[1,] 0.7824846\n",
      "4 4          [,1]\n",
      "[1,] 0.7923388\n",
      "4 5          [,1]\n",
      "[1,] 0.7918751\n"
     ]
    }
   ],
   "source": [
    "# neuralnet modeling, try hidden layers = 1-5\n",
    "library('neuralnet')\n",
    "for (i in c(2,3,4)) {\n",
    "  for (j in c(1,2,3,4,5)){\n",
    "      hidden <- c(i,j)\n",
    "      QSAR_model <- neuralnet(V7 ~ V1 + V2 + V3 + V4 +V5 + V6,\n",
    "                          data = QSAR_train, hidden = c(2,hidden))\n",
    "      model_results <- neuralnet::compute(QSAR_model, QSAR_test[1:6])\n",
    "      predicted_strength <- model_results$net.result\n",
    "      output <- cor(predicted_strength, QSAR_test$V7)\n",
    "      cat(i,j)\n",
    "      print(output)\n",
    "  }  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a723e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When hidden layers=2, hidden nodes = (3,5), correlation get the highest level. Plotting the network again, we see a drastic increase in the number of connections. We can see how this impacted the performance as follows:\n",
    "\n",
    "<img src='Rplot6.png'>\n",
    "\n",
    "* Notice that the reported error (measured again by SSE) has been reduced from 3.03 in the previous model to 2.31 here. Additionally, the number of training steps rose from 949 to 7743, which should come as no surprise given how much more complex the model has become. More complex networks take many more iterations to find the optimal weights.\n",
    "\n",
    "* Applying the same steps to compare the predicted values to the true values, we now obtain a correlation around 0.81, which is a considerable improvement over the previous result of 0.80 with a single hidden layer and node."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
